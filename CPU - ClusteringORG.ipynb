{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-3-a9baa74c867e>, line 226)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a9baa74c867e>\"\u001b[0;36m, line \u001b[0;32m226\u001b[0m\n\u001b[0;31m    ('cluster', kmeans)]\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, pd_rules, index_cols=None,\n",
    "                 categorical_cols=None, drop_cols=None,\n",
    "                 n_clusters_low=2, n_clusters_high=50, n_clusters_stepsize=5, n_processes=None):\n",
    "\n",
    "        self.data = pd_rules.copy()\n",
    "        self.index_cols = index_cols\n",
    "\n",
    "        #preprocessing\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.drop_cols = drop_cols\n",
    "        self.preprocessed = False\n",
    "\n",
    "        self.data_for_kmeans = None  # zscore -> pca -> zscore\n",
    "\n",
    "        #clustering scan\n",
    "        self.n_clusters_low = n_clusters_low\n",
    "        self.n_clusters_high = n_clusters_high\n",
    "        self.n_clusters_stepsize = n_clusters_stepsize\n",
    "        self.n_processes = None\n",
    "\n",
    "        self.models_dict = None\n",
    "        self.inertia_dict = None\n",
    "\n",
    "        #clustering optimal\n",
    "        self.n_clusters_optimal = None\n",
    "        self.kmeans_optimal = None\n",
    "\n",
    "\n",
    "    def train_the_cluster(self):\n",
    "        self.preprocess() #modify self.data\n",
    "\n",
    "        preprocess_dict = self.preprocess_prepare_for_clustering(pca_variance_threshold=0.99)\n",
    "\n",
    "        if self.n_processes is None:\n",
    "            self.find_nclusters()\n",
    "        else:\n",
    "            self.find_nclusters_parallel()\n",
    "\n",
    "        self.find_elbow()  # hard-coded currently\n",
    "        self.train_cluster()\n",
    "\n",
    "        return {\n",
    "            \"prepca_scaler\": preprocess_dict[\"prepca_scaler\"],\n",
    "            \"postpca_scaler\": preprocess_dict[\"postpca_scaler\"],\n",
    "            \"pca\": preprocess_dict[\"pca\"],\n",
    "            \"kmeans\": self.kmeans_optimal\n",
    "        }\n",
    "\n",
    "\n",
    "    def preprocess(self):\n",
    "        if not self.preprocessed: #so train_the_cluster can be called multiple times if necessary\n",
    "            self.preprocess_index()\n",
    "            self.preprocess_dropcols()\n",
    "            self.preprocess_onehot()\n",
    "            self.preprocess_fillnulls()\n",
    "        self.preprocessed = True\n",
    "\n",
    "    def preprocess_index(self):\n",
    "        if self.index_cols is not None:\n",
    "            self.data.set_index(self.index_cols, inplace=True)\n",
    "\n",
    "    def preprocess_onehot(self):\n",
    "        '''one-host encoding\n",
    "        '''\n",
    "        if self.categorical_cols is not None:\n",
    "            df_rest = self.data.drop(self.categorical_cols, axis=1)  # non-categorical columns\n",
    "\n",
    "            df_list = []\n",
    "            for col in self.categorical_cols:\n",
    "                df_onehot = pd.get_dummies(self.data[col], prefix=col)\n",
    "                df_list.append(df_onehot)\n",
    "            df_onehot = pd.concat(df_list, axis=1)  # concat all one-hot encoded cols\n",
    "\n",
    "            df_onehot = pd.concat([df_onehot, df_rest], axis=1)  # concat non-categorical cols\n",
    "\n",
    "            self.data = df_onehot\n",
    "\n",
    "    def preprocess_dropcols(self):\n",
    "        '''Only keep rules columns\n",
    "        Makes assumptions about data columns\n",
    "        '''\n",
    "        if self.drop_cols is not None:\n",
    "            self.data.drop(self.drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    def preprocess_fillnulls(self):\n",
    "        '''Make more sophisticated\n",
    "        '''\n",
    "        self.data.fillna(0, inplace=True)\n",
    "\n",
    "    def preprocess_pca(self, df, pca_variance_threshold=0.99):\n",
    "        pca = PCA()\n",
    "\n",
    "        df_pca = pd.DataFrame(pca.fit_transform(df))\n",
    "        n_components = self.find_pca_ncomponents(pca, pca_variance_threshold)\n",
    "        df_pca = df_pca.iloc[:, :n_components]\n",
    "        pca.n_components = n_components\n",
    "\n",
    "        return pca, df_pca\n",
    "\n",
    "    def find_pca_ncomponents(self, pca_model, pca_variance_threshold):\n",
    "        variance_cumsum = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "\n",
    "        gt_threshold = variance_cumsum[variance_cumsum > pca_variance_threshold]\n",
    "        if len(gt_threshold) == 0:\n",
    "            n_components = len(pca_model.explained_variance_ratio_)\n",
    "        else:\n",
    "            n_components = np.where(variance_cumsum == gt_threshold[0])[0][0]\n",
    "\n",
    "        return n_components\n",
    "\n",
    "    def preprocess_normalize(self, df):\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(df)\n",
    "        df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "        return scaler, df_scaled\n",
    "\n",
    "    def preprocess_prepare_for_clustering(self, pca_variance_threshold=0.99):\n",
    "        scaler_prepca, df_scaled = self.preprocess_normalize(self.data)\n",
    "\n",
    "        pca, df_scaled_pca = self.preprocess_pca(df_scaled, pca_variance_threshold=pca_variance_threshold)\n",
    "\n",
    "        scaler_postpca, df_scaled_pca_scaled = self.preprocess_normalize(df_scaled_pca)\n",
    "\n",
    "        self.data_for_kmeans = df_scaled_pca_scaled\n",
    "\n",
    "        return {'prepca_scaler': scaler_prepca,\n",
    "                'pca': pca,\n",
    "                'postpca_scaler': scaler_postpca,\n",
    "                'df_prepca_scaled': df_scaled,\n",
    "                'df_pca': df_scaled_pca,\n",
    "                'df_postpca_scaled': df_scaled_pca_scaled\n",
    "                }\n",
    "\n",
    "    def find_nclusters(self):\n",
    "        if self.data_for_kmeans is None:\n",
    "            raise AttributeError(\"Please call preprocess_prepare_for_clustering to prepare data for clustering.\")\n",
    "\n",
    "        self.inertia_dict, self.models_dict = {}, {}\n",
    "        for ncl in range(self.n_clusters_low, self.n_clusters_high, self.n_clusters_stepsize):\n",
    "            kmeans = KMeans(n_clusters=ncl)\n",
    "            kmeans.fit(self.data_for_kmeans)\n",
    "\n",
    "            self.inertia_dict[ncl] = kmeans.inertia_\n",
    "            self.models_dict[ncl] = kmeans\n",
    "\n",
    "    def find_nclusters_parallel(self):\n",
    "        manager = mp.Manager()\n",
    "        models_shared_dict = manager.dict()\n",
    "        inertia_shared_dict = manager.dict()\n",
    "\n",
    "        proc_list = []\n",
    "        counter = 0\n",
    "        def run(data, ncl):\n",
    "            kmeans = KMeans(n_clusters=ncl)\n",
    "            kmeans.fit(data)\n",
    "\n",
    "            inertia_shared_dict[ncl] = kmeans.inertia_\n",
    "            models_shared_dict[ncl] = kmeans\n",
    "\n",
    "        for ncl in range(self.n_clusters_low, self.n_clusters_high, self.n_clusters_stepsize):\n",
    "            proc = mp.Process(target=run, args=(self.data, ncl,))\n",
    "            proc.start()\n",
    "            proc_list.append(proc)\n",
    "            counter += 1\n",
    "\n",
    "            if counter % self.n_processes == 0:\n",
    "                [p.join() for p in  proc_list]\n",
    "                proc_list = []\n",
    "\n",
    "        [p.join() for p in proc_list]\n",
    "\n",
    "        self.inertia_dict = dict(inertia_shared_dict)\n",
    "        self.models_dict = dict(models_shared_dict)\n",
    "    \n",
    "    def find_elbow(self):\n",
    "        if self.inertia_dict is None:\n",
    "            raise AttributeError(\"Please run find_nclusters to populated inertia_dict.\")\n",
    "        \n",
    "        self.n_clusters_optimal = 5\n",
    "\n",
    "    def train_cluster(self):\n",
    "        if self.data_for_kmeans is None:\n",
    "            raise AttributeError(\"Please call preprocess_prepare_for_clustering to prepare data for clustering.\")\n",
    "\n",
    "        if not self.n_clusters_optimal:\n",
    "            raise AttributeError(\"Please run find_n_clusters and find_elbow to find optimal n_clusters.\")\n",
    "\n",
    "        self.kmeans_optimal = KMeans(n_clusters=self.n_clusters_optimal).fit(self.data_for_kmeans)\n",
    "\n",
    "    def interpret(self):\n",
    "        if self.kmeans_optimal is None:\n",
    "            raise AttributeError(\"Attribute kmeans_optimal not found. Please train model first.\")\n",
    "\n",
    "        return None\n",
    "        '''\n",
    "        cl = self.kmeans_optimal.predict(self.data_for_kmeans)\n",
    "        df = pd.DataFrame(self.kmeans_optimal.cluster_centers_, columns = self.data_preprocess.columns)\n",
    "        df['cl'] = np.arange(df.shape[0])\n",
    "        model_dtree = DecisionTreeClassifier(max_depth=20)\n",
    "        model_dtree.fit(df.drop('cl', axis=1), df['cl'])\n",
    "        #TODO: generate descriptions from paths\n",
    "        path = model_dtree.decision_path(df.drop('cl', axis=1))\n",
    "        path_index = path.indices[path.indptr[0]:path.indptr[1]]\n",
    "        def get_path(example_row, data, col_names, thresholds):\n",
    "            traversed_nodes = path_nodes.indices[path_nodes.indptr[example_row]:path_nodes.indptr[example_row+1]]\n",
    "            for node in traversed_nodes:\n",
    "                print(f\"Node hit: {features[node]} {col_names[node]} {100*thresholds[node]} {data[example_row][node]}\")\n",
    "        '''\n",
    "\n",
    "\n",
    "    def future_pipeline(self):\n",
    "        '''Might move to pipelines in the future\n",
    "        All intermediate models need to have transform implemented\n",
    "        Final model needs to have predict implemented\n",
    "        '''\n",
    "        prepca_scaler = StandardScaler()\n",
    "        pca = PCA()\n",
    "        postpca_scaler = StandardScaler()\n",
    "        cluster = KMeans()\n",
    "\n",
    "        p = Pipeline([('prepca_scaler', prepca_scaler),\n",
    "                      ('pca', pca),\n",
    "                      ('postpca_scaler', postpca_scaler),\n",
    "                      ('cluster', kmeans)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8bcb5b45f812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, pd_rules, index_cols=None, \\\n\u001b[1;32m      3\u001b[0m                  \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  models_dict=None):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Cluster' is not defined"
     ]
    }
   ],
   "source": [
    "class Inference(Cluster):\n",
    "    def __init__(self, pd_rules, index_cols=None, \\\n",
    "                 categorical_cols=None, drop_cols=None,\n",
    "                 models_dict=None):\n",
    "        \n",
    "        super().__init__(pd_rules, index_cols=index_cols, categorical_cols=categorical_cols, drop_cols=drop_cols)\n",
    "\n",
    "        super().preprocess()\n",
    "\n",
    "        self.models_dict = models_dict\n",
    "        self.load_models()\n",
    "\n",
    "    def load_models(self):\n",
    "        if self.models_dict is None:\n",
    "            raise AttributeError(\"Please specify pre-trained models to load\")\n",
    "\n",
    "        self.prepca_scaler = self.models_dict['prepca_scaler']\n",
    "        self.pca = self.models_dict['pca']\n",
    "        self.postpca_scaler = self.models_dict['postpca_scaler']\n",
    "        self.kmeans_optimal = self.models_dict['kmeans']\n",
    "\n",
    "    def preprocess_prepare_for_clustering(self):\n",
    "        df_prepca_scaled = self.prepca_scaler.transform(self.data)       \n",
    "        \n",
    "        df_pca = pd.DataFrame(self.pca.transform(df_prepca_scaled)).iloc[:,:self.pca.n_components]\n",
    "\n",
    "        df_postpca_scaled = self.postpca_scaler.transform(df_pca)\n",
    "\n",
    "        self.data_for_kmeans = df_postpca_scaled\n",
    "\n",
    "    def predict(self):      \n",
    "        '''Remove df as argument since don't want user to input any other df\n",
    "        '''\n",
    "        self.preprocess_prepare_for_clustering()\n",
    "\n",
    "        cl = self.kmeans_optimal.predict(self.data_for_kmeans)\n",
    "\n",
    "        cl_dict = dict(zip(self.data.index, cl))\n",
    "        \n",
    "        return cl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
